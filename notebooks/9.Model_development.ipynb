{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from model_packages import *\n",
    "from model_config import *\n",
    "from model_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load counter data\n",
    "ne_counters= gpd.read_file(ne_countr_locn_file)\n",
    "ndw_counters= gpd.read_file(ndw_countr_locn_file)\n",
    "crt_counters= gpd.read_file(crt_countr_locn_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of sites from each counter source\n",
    "places_ne= list(ne_counters['counter'].str.replace(' ', '_'))\n",
    "places_ne[19]='Sandwich_Peninsula_Bridge'\n",
    "places_nd= list(ndw_counters['counter'].str.replace(' ', '_'))\n",
    "places_cr= list(crt_counters['Counter'].str.replace(' ', '_'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariate regression model\n",
    "\n",
    "<a id='dest_7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variable selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sites_ne_nd_cr=pd.read_pickle(data_folder+'complete_dataset.pkl')\n",
    "\n",
    "# Data for model training\n",
    "\n",
    "df_for_model_training=df_sites_ne_nd_cr[df_sites_ne_nd_cr['site'].isin(places_ne+places_nd)]\n",
    "\n",
    "\n",
    "print('Total number of sites for model training {}'.format(df_for_model_training['site'].unique().shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "df_for_model_estimation=df_sites_ne_nd_cr[df_sites_ne_nd_cr['site'].isin(places_cr)]\n",
    "\n",
    "\n",
    "print('Total number of sites for model estimation {}'.format(df_for_model_estimation['site'].unique().shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Some descriptive stat to choose reference category for land use and land habitat features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_classf=df_for_model_training[['site',\"land_type_labels\",\"habitat_type_labels\"]].drop_duplicates()\n",
    "\n",
    "print(site_classf['habitat_type_labels'].value_counts())\n",
    "\n",
    "print(site_classf['land_type_labels'].value_counts())\n",
    "\n",
    "# Not all classes are present in the training data\n",
    "#df_for_model_training.value_counts(\"land_type_labels\")\n",
    "#df_for_model_training.value_counts(\"habitat_type_labels\")\n",
    "\n",
    "sns.violinplot(data=df_for_model_training, y=\"people_counter_data\",x=\"land_type_labels\",hue='habitat_type_labels')\n",
    "plt.ylabel('People counter')\n",
    "plt.show()\n",
    "\n",
    "sns.violinplot(data=df_for_model_training, y=\"total_trip_count\", x=\"land_type_labels\",\\\n",
    "            hue=\"habitat_type_labels\")\n",
    "plt.ylabel('Strava count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_classf=df_for_model_estimation[['site',\"land_type_labels\",\"habitat_type_labels\"]].drop_duplicates()\n",
    "\n",
    "print(site_classf['habitat_type_labels'].value_counts())\n",
    "\n",
    "print(site_classf['land_type_labels'].value_counts())\n",
    "\n",
    "df_for_model_estimation.value_counts(\"land_type_labels\")\n",
    "df_for_model_estimation.value_counts(\"habitat_type_labels\")\n",
    "\n",
    "sns.violinplot(data=df_for_model_estimation, y=\"total_trip_count\", x=\"land_type_labels\",\\\n",
    "            hue=\"habitat_type_labels\")\n",
    "plt.ylabel('Strava count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To deal with class imbalance- we combine the lowest minority classes into one class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hbt_dict={'Grassland_woodland_bareground':'Grass_wood_bareground_coast',\\\n",
    "          'Grassland_woodland_coastal':'Grass_wood_bareground_coast'}\n",
    "\n",
    "lnd_dict={'rural_mixed_settings':'urban_and_rural_setings',\\\n",
    "          'urban_settings':'urban_and_rural_setings'}\n",
    "\n",
    "\n",
    "\n",
    "df_for_model_training['habitat_type_labels'].replace(hbt_dict,inplace=True)\n",
    "\n",
    "df_for_model_training['land_type_labels'].replace(lnd_dict,inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "df_for_model_estimation['habitat_type_labels'].replace(hbt_dict,inplace=True)\n",
    "\n",
    "df_for_model_estimation['land_type_labels'].replace(lnd_dict,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_classf=df_for_model_training[['site',\"land_type_labels\",\"habitat_type_labels\"]].drop_duplicates()\n",
    "\n",
    "print(site_classf['habitat_type_labels'].value_counts())\n",
    "\n",
    "print(site_classf['land_type_labels'].value_counts())\n",
    "\n",
    "# Not all classes are present in the training data\n",
    "\n",
    "#df_for_model_training.value_counts(\"land_type_labels\")\n",
    "#df_for_model_training.value_counts(\"habitat_type_labels\")\n",
    "\n",
    "sns.violinplot(data=df_for_model_training, y=\"people_counter_data\",x=\"land_type_labels\",hue='habitat_type_labels')\n",
    "plt.ylabel('People counter')\n",
    "plt.show()\n",
    "\n",
    "sns.violinplot(data=df_for_model_training, y=\"total_trip_count\", x=\"land_type_labels\",\\\n",
    "            hue=\"habitat_type_labels\")\n",
    "plt.ylabel('Strava count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_classf=df_for_model_estimation[['site',\"land_type_labels\",\"habitat_type_labels\"]].drop_duplicates()\n",
    "\n",
    "print(site_classf['habitat_type_labels'].value_counts())\n",
    "\n",
    "print(site_classf['land_type_labels'].value_counts())\n",
    "\n",
    "# Not all classes are present in the training data\n",
    "\n",
    "#df_for_model_training.value_counts(\"land_type_labels\")\n",
    "#df_for_model_training.value_counts(\"habitat_type_labels\")\n",
    "\n",
    "\n",
    "sns.violinplot(data=df_for_model_estimation, y=\"total_trip_count\", x=\"land_type_labels\",\\\n",
    "            hue=\"habitat_type_labels\")\n",
    "plt.ylabel('Strava count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load demographic features\n",
    "demo_data= pd.read_pickle(data_folder+'socio_economic_data_all_sites.pkl')\n",
    "# load POI features\n",
    "pois_data= pd.read_pickle(data_folder+'pois_data_all_sites.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pois_demo_all_sites=pois_data.merge(demo_data,left_on=['site'],\\\n",
    "                                               right_on=['counter'],how='inner')\n",
    "\n",
    "\n",
    "df_pois_demo_all_sites=df_pois_demo_all_sites._get_numeric_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trgt_ftrs=['people_counter_data']\n",
    "\n",
    "\n",
    "dynmc_ftrs=['total_trip_count']\n",
    "\n",
    "wthr_ftrs=['tavg']\n",
    "\n",
    "othr_ftrs=['Mean_dog_occupancy']\n",
    "           \n",
    "    \n",
    "lnd_ftrs=['land_type_labels_'+x for x in list(df_for_model_training['land_type_labels'].unique())]\n",
    "\n",
    "hbt_ftrs=['habitat_type_labels_'+x for x in list(df_for_model_training['habitat_type_labels'].unique())]\n",
    "    \n",
    "catg_ftrs=['Date', 'site']\n",
    "           \n",
    "\n",
    "\n",
    "\n",
    "natr_ftrs_non_corr=['accessible_green_space_area','PROW_Total_length_km','waterside_length_km']\n",
    "\n",
    "corr_cens_pois=df_pois_demo_all_sites.columns.tolist()\n",
    "\n",
    "#non_corr_cens_pois=sel_demo_pois_vrbls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([df_for_model_training,df_for_model_estimation]).reset_index(drop=True).\\\n",
    "to_pickle(data_folder+'complete_dataset.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_model_training=pd.get_dummies(df_for_model_training,columns=['land_type_labels'])\n",
    "\n",
    "df_for_model_training=pd.get_dummies(df_for_model_training,columns=['habitat_type_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_model_estimation=pd.get_dummies(df_for_model_estimation,columns=['land_type_labels'])\n",
    "\n",
    "df_for_model_estimation=pd.get_dummies(df_for_model_estimation,columns=['habitat_type_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_model_training.to_pickle(data_folder+'df_for_model_training.pkl')\n",
    "\n",
    "df_for_model_estimation.to_pickle(data_folder+'df_for_model_estimation.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression model with regularisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val= train_test_split(df_for_model_training, test_size=0.3, random_state=0,\\\n",
    "                                   stratify=df_for_model_training[['site']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle(data_folder+'sites_training_data.pkl')\n",
    "\n",
    "df_val.to_pickle(data_folder+'sites_validation_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftrs_to_keep=trgt_ftrs+dynmc_ftrs+wthr_ftrs+othr_ftrs+natr_ftrs_non_corr+\\\n",
    "lnd_ftrs+hbt_ftrs+catg_ftrs+corr_cens_pois\n",
    "\n",
    "ftrs_to_keep=[x for x in ftrs_to_keep if x in list(df_train.columns)]\n",
    "print(ftrs_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom palette\n",
    "cmap = sns.diverging_palette(h_neg= 250, h_pos= 15, s=75, l=40, n=9, center=\"light\", as_cmap=True)\n",
    "\n",
    "\n",
    "# Compute corr matrix\n",
    "matrix = pois_data.corr(method=\"pearson\").dropna(how='all',axis=1).dropna(how='all',axis=0)\n",
    "# Create a mask\n",
    "\n",
    "# matrix=matrix[abs(matrix)>0.7].fillna(0.)\n",
    "# mask = np.triu(np.ones_like(matrix, dtype=bool))\n",
    "\n",
    "plt= sns.clustermap(matrix, annot=True,figsize=(10,10),\\\n",
    "               dendrogram_ratio=0.15,cmap=cmap,square=True,fmt=\".2f\",annot_kws={\"size\": 10}, vmin=-1, vmax=1)\n",
    "\n",
    "\n",
    "plt.savefig(f\"./outputs/poi_heatmap.png\", format= 'png', dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clstr_labls=['habitat_type_labels_Grassland_woodland_wetland','land_type_labels_rural_settings']\n",
    "\n",
    "\n",
    "\n",
    "ftrs_to_keep=[x for x in ftrs_to_keep if x not in clstr_labls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each ftr, calculate VIF and save in dataframe\n",
    "vif = pd.DataFrame()\n",
    "\n",
    "df_train_num=df_train[[x for x in ftrs_to_keep if x not in [target]]].select_dtypes(include=np.number)\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(df_train_num.values, i) for \\\n",
    "                     i in range(df_train_num.shape[1])]\n",
    "vif[\"features\"] = df_train_num.columns\n",
    "\n",
    "\n",
    "print(vif)\n",
    "\n",
    "print('+'*100)\n",
    "\n",
    "ftrs_to_chck=[x for x in ftrs_to_keep if x not in [target,'total_trip_count','tavg']]\n",
    "              \n",
    "df_train_num_remv_multi_coll=calculate_vif_(df_train[ftrs_to_chck].select_dtypes(include=np.number),\\\n",
    "                                            thresh=10)[0]\n",
    "\n",
    "vif = pd.DataFrame()\n",
    "vif[\"VIF Factor\"] = [variance_inflation_factor(df_train_num_remv_multi_coll.values, i) for \\\n",
    "                     i in range(df_train_num_remv_multi_coll.shape[1])]\n",
    "vif[\"features\"] = df_train_num_remv_multi_coll.columns\n",
    "\n",
    "\n",
    "#Vif removed features\n",
    "low_vif_ftrs_df_train_num=list(vif.features.values)\n",
    "\n",
    "# possibly correlated features\n",
    "#low_vif_ftrs_df_train_num=ftrs_to_chck\n",
    "              \n",
    "low_vif_ftrs_df_train_num=low_vif_ftrs_df_train_num+['total_trip_count','tavg']\n",
    "print(low_vif_ftrs_df_train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a custom palette\n",
    "cmap = sns.diverging_palette(250, 15, s=75, l=40, n=9, center=\"light\", as_cmap=True)\n",
    "\n",
    "\n",
    "# Compute corr matrix\n",
    "matrix = df_for_model_training[low_vif_ftrs_df_train_num+[target]].corr(method=\"pearson\").dropna(how='all',axis=1).dropna(how='all',axis=0)\n",
    "# Create a mask\n",
    "\n",
    "matrix=matrix[abs(matrix)>0.5].fillna(0.)\n",
    "mask = np.triu(np.ones_like(matrix, dtype=bool))\n",
    "\n",
    "sns.clustermap(matrix.sort_values(by='people_counter_data',ascending=False), annot=True,figsize=(10,10),\\\n",
    "               dendrogram_ratio=0.1,cmap='RdBu',square=True,fmt=\".2f\",annot_kws={\"size\": 6})\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target='people_counter_data'\n",
    "\n",
    "\n",
    "ftrs_to_keep=low_vif_ftrs_df_train_num+[target]\n",
    "\n",
    "ftrs_to_keep_df=pd.DataFrame(ftrs_to_keep).rename(columns={0:'features_for_model'})\n",
    "\n",
    "ftrs_to_keep_df.to_pickle(data_folder+'ftrs_to_keep_df.pkl')\n",
    "\n",
    "\n",
    "# remove feature strava \n",
    "#ftrs_to_keep=[x for x in ftrs_to_keep if x not in ['total_trip_count','tavg']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ftrs_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df_train[ftrs_to_keep].copy()#ne_counter_strava_mdl_amenity_wthr_pop\n",
    "\n",
    "\n",
    "features = [x for x in df.columns if x not in ['Date','site',target]]\n",
    "\n",
    "categorical_columns = []\n",
    "numerical_columns = features\n",
    "\n",
    "preprocessor = make_column_transformer(\n",
    "    (OneHotEncoder(drop=\"if_binary\"), categorical_columns),\n",
    "    remainder=\"passthrough\",\n",
    "    verbose_feature_names_out=False,  \n",
    ")\n",
    "\n",
    "np.random.seed(8)\n",
    "scaler = RobustScaler()#/ \n",
    "\n",
    "rgr=ElasticNet()\n",
    "param_distributions_rgrn={'alpha' : np.linspace(0.05, 5, 100),'l1_ratio':np.linspace(0.01, 1, 50)}\n",
    "            \n",
    "    \n",
    "# if scaling the data    \n",
    "scaler.fit(df[features])  \n",
    "        \n",
    "df[features] = scaler.transform(df[features])\n",
    "\n",
    "\n",
    "# save the scaler\n",
    "dump(scaler, open(data_folder+'scaler.pkl', 'wb'))\n",
    "\n",
    "\n",
    "cv=RepeatedKFold(n_splits=5, n_repeats=10, random_state=1)\n",
    "        \n",
    "rs = RandomizedSearchCV(estimator=rgr, n_iter=2000,param_distributions=param_distributions_rgrn,cv=cv,\\\n",
    "                        scoring='explained_variance')\n",
    "\n",
    "\n",
    "print(df[features].values.shape)\n",
    "print(len(features))\n",
    "X_train = df[features].values.reshape(-1, len(features))\n",
    "y_train = df[target].values\n",
    "\n",
    "rs.fit(X_train, y_train)\n",
    "            \n",
    "     \n",
    "print(rs.best_score_)\n",
    "            \n",
    "print(rs.best_params_)     \n",
    "\n",
    "\n",
    "\n",
    "# Getting the best estimator\n",
    "\n",
    "alpha_reg=rs.best_params_.get('alpha')\n",
    "l1_reg=rs.best_params_.get('l1_ratio')\n",
    "model_regularisation = make_pipeline(preprocessor,\n",
    "    TransformedTargetRegressor(\n",
    "        regressor=rs.best_estimator_\n",
    "    ),\n",
    ")\n",
    "\n",
    "model_regularisation.fit(df[features], df[target])\n",
    "\n",
    "# save the model\n",
    "dump(model_regularisation, open(data_folder+'model_regularisation.pkl', 'wb'))\n",
    "\n",
    "y_pred = model_regularisation.predict(X_train)\n",
    "\n",
    "y_pred =[max(0,x) for x in y_pred]\n",
    "\n",
    "mae = median_absolute_error(y_train, y_pred)\n",
    "string_score = f\"MAE on training set: {mae:.2f}\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.scatter(y_train, y_pred)\n",
    "ax.plot(transform=ax.transAxes, ls=\"--\", c=\"red\")\n",
    "plt.text(3, 20, string_score)\n",
    "plt.title(\"ElasticNet, regularization with alpha {} and l_1 ratio {}\".format(np.round(alpha_reg,2),\\\n",
    "                                                                             np.round(l1_reg,2)))\n",
    "plt.ylabel(\"Model predictions\")\n",
    "plt.xlabel(\"Truths\")\n",
    "\n",
    "print('r2_score on train data {}'.format(r2_score(y_train,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_regularisation\n",
    "\n",
    "rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model\n",
    "\n",
    "model_regularisation = pickle.load(open(data_folder+'model_regularisation.pkl', 'rb'))\n",
    "# load the scaler\n",
    "scaler = pickle.load(open(data_folder+'scaler.pkl', 'rb'))\n",
    "\n",
    "df_valid=df_val[ftrs_to_keep].copy() \n",
    "\n",
    "df_valid[features] = scaler.transform(df_valid[features])\n",
    "\n",
    "X_valid = df_valid[features].values.reshape(-1, len(features))\n",
    "\n",
    "y_valid = df_valid[target].values\n",
    "\n",
    "y_pred_valid = model_regularisation.predict(X_valid)\n",
    "\n",
    "y_pred_valid =[max(0,x) for x in y_pred_valid]\n",
    "\n",
    "mae = median_absolute_error(y_valid, y_pred_valid)\n",
    "string_score = f\"MAE on validation set: {mae:.2f}\"\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.scatter(y_valid, y_pred_valid)\n",
    "ax.plot(transform=ax.transAxes, ls=\"--\", c=\"red\")\n",
    "plt.text(3, 20, string_score)\n",
    "plt.title(\"ElasticNet, regularization with alpha {} and l_1 ratio {}\".format(np.round(alpha_reg,2),\\\n",
    "                                                                             np.round(l1_reg,2)))\n",
    "plt.ylabel(\"Model predictions\")\n",
    "plt.xlabel(\"Truths\")\n",
    "\n",
    "print('r2_score on valid data {}'.format(r2_score(y_valid,y_pred_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Stability of predictors**\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html\n",
    "\n",
    "We can check the coefficient variability through cross-validation: it is a form of data perturbation (related to resampling).\n",
    "\n",
    "If coefficients vary significantly when changing the input dataset their robustness is not guaranteed, and they should probably be interpreted with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = model_regularisation[:-1].get_feature_names_out()\n",
    "\n",
    "coefs = pd.DataFrame(\n",
    "    model_regularisation[-1].regressor_.coef_,\n",
    "    columns=[\"Coefficients\"],\n",
    "    index=feature_names,\n",
    ")\n",
    "\n",
    "coefs.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs.plot.barh(figsize=(9, 7))\n",
    "plt.title(\"ElasticNet, regularization with alpha {} and l_1 ratio {}\".format(np.round(alpha_reg,2),\\\n",
    "                                                                             np.round(l1_reg,2)))\n",
    "plt.axvline(x=0, color=\".5\")\n",
    "plt.xlabel(\"Raw coefficient values\")\n",
    "plt.subplots_adjust(left=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_preprocessed = pd.DataFrame(\n",
    "    model_regularisation[:-1].transform(X_train), columns=feature_names\n",
    ")\n",
    "\n",
    "X_train_preprocessed.std(axis=0).plot.barh(figsize=(9, 7))\n",
    "plt.title(\"Feature ranges\")\n",
    "plt.xlabel(\"Std. dev. of feature values\")\n",
    "plt.subplots_adjust(left=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs = pd.DataFrame(\n",
    "    model_regularisation[-1].regressor_.coef_ * X_train_preprocessed.std(axis=0),\n",
    "    columns=[\"Coefficient importance\"],\n",
    "    index=feature_names,\n",
    ")\n",
    "coefs.plot(kind=\"barh\", figsize=(9, 7))\n",
    "plt.xlabel(\"Coefficient values corrected by the feature's std. dev.\")\n",
    "plt.title(\"ElasticNet, regularization with alpha {} and l_1 ratio {}\".format(np.round(alpha_reg,2),\\\n",
    "                                                                             np.round(l1_reg,2)))\n",
    "plt.axvline(x=0, color=\".5\")\n",
    "plt.subplots_adjust(left=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = RepeatedKFold(n_splits=5, n_repeats=10, random_state=0)\n",
    "cv_model = cross_validate(\n",
    "    model_regularisation,\n",
    "    df[features],\n",
    "    df[target],\n",
    "    cv=cv,\n",
    "    return_estimator=True,\n",
    "    n_jobs=2,\n",
    ")\n",
    "\n",
    "coefs = pd.DataFrame(\n",
    "    [\n",
    "        est[-1].regressor_.coef_ * est[:-1].transform(df.iloc[train_idx]).std(axis=0)\n",
    "        for est, (train_idx, _) in zip(cv_model[\"estimator\"], cv.split(df[features], df[target]))\n",
    "    ],\n",
    "    columns=feature_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefs=coefs.reindex(sorted(coefs.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.stripplot(data=coefs, orient=\"h\", color=\"k\", alpha=0.5)\n",
    "sns.boxplot(data=coefs, orient=\"h\", color=\"cyan\", saturation=0.5, whis=10)\n",
    "plt.axvline(x=0, color=\".5\")\n",
    "plt.xlabel(\"Coefficient importance\")\n",
    "plt.title(\"Coefficients importance and their variability\")\n",
    "plt.suptitle(\"ElasticNet, regularization with alpha {} and l_1 ratio {}\".format(np.round(alpha_reg,2),\\\n",
    "                                                                             np.round(l1_reg,2)))\n",
    "plt.subplots_adjust(left=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear regression without regularisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y=df_train[features].copy(), df_train[target].copy()\n",
    "\n",
    "X_valid,y_valid=df_val[features].copy(), df_val[target].copy()\n",
    "\n",
    "X_regr=X.copy()\n",
    "y_regr=y.copy()\n",
    "\n",
    "X_regr_valid=X_valid.copy()\n",
    "y_regr_valid=y_valid.copy()\n",
    "\n",
    "\n",
    "sclng_flg=True\n",
    "\n",
    "if sclng_flg:\n",
    "    scaler_lin = StandardScaler()#/ \n",
    "    scaler_lin.fit(X_regr)  # \n",
    "    X_regr = scaler_lin.transform(X_regr)\n",
    "    \n",
    "    X_regr_valid = scaler_lin.transform(X_regr_valid)\n",
    "\n",
    "\n",
    "ols = linear_model.LinearRegression()\n",
    "model = ols.fit(X_regr, y_regr)\n",
    "\n",
    "print('Features                :  %s' % features)\n",
    "print('Regression Coefficients : ', [round(item, 4) for item in model.coef_])\n",
    "print('R-squared               :  %.4f' % model.score(X_regr, y_regr))\n",
    "print('Y-intercept             :  %.4f' % model.intercept_)\n",
    "print('')\n",
    "\n",
    "print('+'*200)\n",
    "\n",
    "\n",
    "y_pred=[max(0,x) for x in model.predict(X_regr)]\n",
    "\n",
    "\n",
    "print('r2_score on train data {}'.format(r2_score(y_regr,y_pred)))\n",
    "\n",
    "plt.scatter(y_regr,y_pred)\n",
    "\n",
    "y_pred_valid=[max(0,x) for x in model.predict(X_regr_valid)]\n",
    "\n",
    "plt.scatter(y_regr_valid,y_pred_valid)\n",
    "\n",
    "print('r2_score on valid data {}'.format(r2_score(y_regr_valid,y_pred_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add constant to predictor variables\n",
    "x = sm.add_constant(X_regr)\n",
    "\n",
    "#fit linear regression model\n",
    "model = sm.OLS(y_regr, x).fit()\n",
    "\n",
    "#view model summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting regression coefficients as table\n",
    "lm = pg.linear_regression(pd.DataFrame(X_regr,columns=features), y_regr)\n",
    "\n",
    "lm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Latex table of model coefficients\n",
    "\n",
    "# ft_table= lm.copy()\n",
    "# ft_table.names=ft_table.names.str.replace(\"_\", \" \")\n",
    "\n",
    "# print(ft_table.to_latex(index=False,\n",
    "#                   formatters={\"name\": str.upper},\n",
    "#                   float_format=\"{:.2f}\".format,\n",
    "#                   longtable= True\n",
    "#                   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression Model performance using cross validation\n",
    "\n",
    "#https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "\n",
    "print(metrics.SCORERS.keys())\n",
    "rgr_pip = make_pipeline(scaler_lin,LinearRegression())\n",
    "\n",
    "\n",
    "cv = ShuffleSplit(n_splits=10, test_size=0.25, random_state=0)\n",
    "\n",
    "scores=cross_val_score(rgr_pip, X,y, cv=cv,scoring='explained_variance')\n",
    "\n",
    "print(scores)\n",
    "\n",
    "print(np.mean(scores))\n",
    "\n",
    "scores=cross_val_score(rgr_pip, X,y, cv=cv,scoring='neg_mean_absolute_error')\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL COMPARISONS**\n",
    "\n",
    "*We compare performance of different models*\n",
    "\n",
    "<a id='dest_addtnl'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=pd.read_pickle(data_folder+'sites_training_data.pkl')\n",
    "\n",
    "kept_ftrs_train=dataset[['Date','site',target]]\n",
    "\n",
    "ftrs_to_keep=pd.read_pickle(data_folder+'ftrs_to_keep_df.pkl')['features_for_model'].values\n",
    "\n",
    "dataset=dataset[ftrs_to_keep]\n",
    "\n",
    "\n",
    "\n",
    "print(ftrs_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform dataset (transform, bin and create dummy variables) and split the dataset. In addition,\n",
    "# we are logging experiments and plots for those experiment to be viewed later with MLflow. \n",
    "reg_fcr = setup(data=dataset, target=target, session_id=786, transformation=False,\\\n",
    "                normalize=False, train_size=0.7,\\\n",
    "                numeric_features=[x for x in list(dataset.select_dtypes(include=np.number).columns)\\\n",
    "                                  if x not in [target]],\\\n",
    "                remove_outliers=False,\\\n",
    "                outliers_threshold=0.05, remove_multicollinearity=True,\\\n",
    "                multicollinearity_threshold=0.9,feature_selection=False,\\\n",
    "                ignore_features=['Date','site'],\\\n",
    "                polynomial_features=False,feature_selection_method='classic',\\\n",
    "                pca=False, log_experiment=True,\\\n",
    "                experiment_name='reg_experiments', log_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can specify which models to compare or let the best model be selected \n",
    "#top5 = compare_models(n_select=5, exclude=(['ransac', 'knn']), sort='RMSE', fold=5)\n",
    "\n",
    "top5 =compare_models(n_select=5, include=(['lr','ridge','lasso','en','br','kr']),sort='R2', fold=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pull().sort_values(by='R2', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_top5 = [tune_model(i, n_iter=120, optimize='R2', fold=5) for i in top5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blender_specific = blend_models(estimator_list=tuned_top5[0:], fold=5, optimize='R2', choose_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is a view of the model parameters. \n",
    "blender_specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stacking models is an ensemble method of using meta learning, where a meta model is created \n",
    "# using multiple base estimators to generate the final prediction.\n",
    "# Let's try building a stacking model from our top 5 models and evaluate the results. \n",
    "stacker_specific = stack_models(estimator_list=tuned_top5[1:], meta_model=tuned_top5[0],\\\n",
    "                                fold=5,optimize='R2', choose_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is a view of the model parameters. \n",
    "stacker_specific"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use Pycaret's built in plot_model() function to generate side-by-side plots: the Residuals chart, Prediction Error and Cross Validation (learning) charts. Let's compare the Blend and Stack model plots in a side-by-side comparison. \n",
    "#fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(2,2,1)\n",
    "plot_model(blender_specific, plot='residuals', verbose=False, scale=1.1)\n",
    "ax = fig.add_subplot(2,2,2)\n",
    "plot_model(stacker_specific, plot='residuals', verbose=False, scale=1.1)\n",
    "ax = fig.add_subplot(2,2,3)\n",
    "plot_model(blender_specific, plot='error', verbose=False, scale=1.1)\n",
    "ax = fig.add_subplot(2,2,4)\n",
    "plot_model(stacker_specific, plot='error',  verbose=False, scale=1.1)\n",
    "#ax = fig.add_subplot(3,2,5)\n",
    "#plot_model(blender_specific, plot='learning',verbose=False, scale=1.1)\n",
    "#ax = fig.add_subplot(3,2,6)\n",
    "#plot_model(stacker_specific, plot='learning', verbose=False, scale=1.1)\n",
    "#plt.savefig('plots_blender_vs_stacker.png', dpi=300, pad_inches=0.25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can execute the predict_model() function to use the model to generate the predicted values. \n",
    "pred_tunded_blender = predict_model(blender_specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can execute the predict_model() function to use the model to generate the predicted values. \n",
    "pred_tunded_stacker = predict_model(stacker_specific)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Blend model seems to perform better in both our train and test so let us finalize it.\n",
    "#The finalize_model() function trains the model on the entire dataset. \n",
    "finalize_blender = finalize_model(blender_specific)#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(finalize_blender, data_folder+'pycaret_regression_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pycaret_model = load_model(data_folder+'pycaret_regression_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predict_model() can be executed with the final blend model over \n",
    "# the entire dataset and saved to a csv file. \n",
    "pred_on_train = predict_model(pycaret_model, data=dataset[[x for x in dataset.columns if x not in ['people_counter_data']]])\n",
    "\n",
    "pred_on_train['prediction_label']=pred_on_train['prediction_label'].clip(0)\n",
    "\n",
    "pred_on_train=pd.concat([pred_on_train.reset_index(drop=True),kept_ftrs_train.reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pred_on_train['people_counter_data'],pred_on_train['prediction_label'])\n",
    "\n",
    "print('r2_score is {}'.format(r2_score(pred_on_train[target],pred_on_train['prediction_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_valid=pd.read_pickle(data_folder+'sites_validation_data.pkl').reset_index(drop=True)\n",
    "\n",
    "kept_ftrs_valid=dataset_valid[['Date','site',target]]\n",
    "\n",
    "dataset_valid=dataset_valid[list(ftrs_to_keep)]#+['Date','site']]\n",
    "\n",
    "y_valid=dataset_valid[target]\n",
    "\n",
    "dataset_valid=dataset_valid[[x for x in dataset_valid.columns if x not in [target]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The predict_model() can be executed with the final blend model over \n",
    "# the entire dataset and saved to a csv file. \n",
    "pred_on_val = predict_model(pycaret_model, data=dataset_valid)\n",
    "\n",
    "pred_on_val['prediction_label']=pred_on_val['prediction_label'].clip(0)\n",
    "\n",
    "pred_on_val=pd.concat([pred_on_val.reset_index(drop=True),kept_ftrs_valid.reset_index(drop=True)],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(pred_on_val[target],pred_on_val['prediction_label'])\n",
    "print('r2_score is {}'.format(r2_score(pred_on_val[target],pred_on_val['prediction_label'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_on_train.to_pickle(data_folder+'pred_on_train_data.pkl')\n",
    "\n",
    "pred_on_val.to_pickle(data_folder+'pred_on_val_data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtcns_sites=pred_on_val.set_index('site')\n",
    "\n",
    "predtcns_sites['Date']=pd.to_datetime(predtcns_sites['Date'])\n",
    "\n",
    "predtcns_sites=predtcns_sites.sort_values(by='Date')\n",
    "\n",
    "\n",
    "#sns.scatterplot(data=predtcns_sites, x=\"people_counter_data\", y=\"prediction_label\", hue=\"site\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(predtcns_sites, x=\"people_counter_data\", y=\"prediction_label\", color=predtcns_sites.index)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data wrangling for visualisation \n",
    "\n",
    "predtcns_sites=predtcns_sites.reset_index()\n",
    "\n",
    "\n",
    "predtcns_sites=pd.melt(predtcns_sites,id_vars=['site','Date'],var_name='visitors', value_name='value')\n",
    "\n",
    "predtcns_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth for all sites and dates\n",
    "predtcns_sites_a=predtcns_sites[predtcns_sites['visitors']=='people_counter_data']\n",
    "predtcns_sites_a['site']=predtcns_sites_a['site']+'-'+predtcns_sites_a['visitors']\n",
    "\n",
    "del predtcns_sites_a['visitors']\n",
    "\n",
    "predtcns_sites_a=predtcns_sites_a.sort_values(by=['Date','site']).reset_index(drop=True)\n",
    "\n",
    "print(predtcns_sites_a)\n",
    "\n",
    "# Predictions for all sites and dates\n",
    "predtcns_sites_b=predtcns_sites[predtcns_sites['visitors']=='prediction_label']\n",
    "predtcns_sites_b['site']=predtcns_sites_b['site']+'-'+predtcns_sites_b['visitors']\n",
    "\n",
    "del predtcns_sites_b['visitors']\n",
    "\n",
    "predtcns_sites_b=predtcns_sites_b.sort_values(by=['Date','site']).reset_index(drop=True)\n",
    "\n",
    "print(predtcns_sites_b)\n",
    "\n",
    "# Strava count for all sites and dates\n",
    "predtcns_sites_c=predtcns_sites[predtcns_sites['visitors']=='total_trip_count']\n",
    "predtcns_sites_c['site']=predtcns_sites_c['site']+'-'+predtcns_sites_c['visitors']\n",
    "\n",
    "del predtcns_sites_c['visitors']\n",
    "\n",
    "predtcns_sites_c=predtcns_sites_c.sort_values(by=['Date','site']).reset_index(drop=True)\n",
    "\n",
    "print(predtcns_sites_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predtcns_sites_a.groupby('Date')['value'].sum().plot(style='-o',label='Actual count')\n",
    "\n",
    "predtcns_sites_b.groupby('Date')['value'].sum().plot(style='--',c='r',label='Predicted count')\n",
    "\n",
    "#(predtcns_sites_c.groupby('Date')['value'].sum()*10).plot(style='--',c='g',label='Strava count (scaled)')\n",
    "\n",
    "\n",
    "\n",
    "plt.ylabel('Visits')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1 = px.line(predtcns_sites_a, x=\"Date\", color=\"site\",\n",
    "             y='value',\n",
    "             title=\"True count\",markers=True\n",
    "            )\n",
    "fig1.update_xaxes(dtick=\"M1\",tickformat=\"%b\\n%Y\")\n",
    "#fig.show()\n",
    "fig2 = px.line(predtcns_sites_b, x=\"Date\", color=\"site\",\n",
    "             y='value',\n",
    "             title=\"Predicted count\",markers=True,\n",
    "            )\n",
    "fig2.update_xaxes(dtick=\"M1\",tickformat=\"%b\\n%Y\")\n",
    "#fig.show()\n",
    "fig2.update_traces(patch={\"line\": {\"dash\": 'dot'}}) \n",
    "\n",
    "fig3 = go.Figure(data=fig1.data + fig2.data)\n",
    "fig3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can use the Pycaret's built-in plot_model() function to generate Residuals and\n",
    "# Error plots for the finalized blend model. \n",
    "fig = plt.figure(figsize=(5,10))\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "plot_model(pycaret_model, plot='residuals',  verbose=False, scale=1.1)\n",
    "ax = fig.add_subplot(2,1,2)\n",
    "plot_model(pycaret_model, plot='error',  verbose=False, scale=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.DataFrame({'Feature': get_config('X_train').columns, 'Value' : finalize_blender.coef_}).\\\n",
    "#sort_values(by='Value', ascending=False).set_index('Feature').plot(kind='barh')\n",
    "\n",
    "#pd.DataFrame({'Feature': get_config('X_train').columns, 'Value' : stacker_specific.coef_}).\\\n",
    "#sort_values(by='Value', ascending=False).set_index('Feature').plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Clustering on sites to identify similar sites (based on training data and only on static attributes)**\n",
    "\n",
    "The idea is to identify nearest beighbour (similar site) for each site in the unseen sites (river and canal trust data for example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in all the static features in the data \n",
    "all_sites_model=pd.read_pickle(data_folder+'complete_dataset.pkl')\n",
    "\n",
    "counter_ne=pd.read_pickle(data_folder+'counter_ne.pkl')\n",
    "\n",
    "counter_nd=pd.read_pickle(data_folder+'counter_nd.pkl')\n",
    "\n",
    "counter_cr=pd.read_pickle(data_folder+'counter_cr.pkl')\n",
    "\n",
    "\n",
    "all_sites_model=pd.get_dummies(all_sites_model,columns=['habitat_type_labels'])\n",
    "\n",
    "all_sites_model=pd.get_dummies(all_sites_model,columns=['land_type_labels'])\n",
    "\n",
    "ftrs_to_keep=pd.read_pickle(data_folder+'ftrs_to_keep_df.pkl')['features_for_model'].values\n",
    "\n",
    "\n",
    "ftrs_to_be_used=[x for x in ftrs_to_keep if x not in ['tavg','total_trip_count','people_counter_data']]+['site']\n",
    "\n",
    "print(ftrs_to_be_used)\n",
    "\n",
    "all_sites_model=all_sites_model[ftrs_to_be_used].drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "print(all_sites_model.shape)\n",
    "\n",
    "all_sites_num_colmns=list(all_sites_model._get_numeric_data().columns)\n",
    "\n",
    "\n",
    "#scale the data\n",
    "\n",
    "scaler_data=MinMaxScaler()\n",
    "\n",
    "all_sites_model[all_sites_num_colmns]=scaler_data.fit_transform(all_sites_model[all_sites_num_colmns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sites=list(pd.read_pickle(data_folder+'df_for_model_training.pkl')['site'].unique())\n",
    "\n",
    "estm_sites=list(pd.read_pickle(data_folder+'df_for_model_estimation.pkl')['site'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_train=all_sites_model[all_sites_model['site'].isin(training_sites)].reset_index(drop=True)\n",
    "\n",
    "ftrs_train=sites_train[[x for x in sites_train.columns if x not in ['site']]]\n",
    "\n",
    "\n",
    "sites_estimation=all_sites_model[all_sites_model['site'].isin(estm_sites)].reset_index(drop=True)\n",
    "\n",
    "\n",
    "ftrs_estm=sites_estimation[[x for x in sites_estimation.columns if x not in ['site']]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a K-D tree on the data points in the train set \n",
    "# and find the nearest neighbour for each site in unseen data\n",
    "A = ftrs_train.values\n",
    "tree = spatial.KDTree(A)\n",
    "\n",
    "str_neghbrs=[]\n",
    "\n",
    "negbrs_dist=[]\n",
    "\n",
    "negbrs_valus=[]\n",
    "for indx in range(ftrs_estm.shape[0]):\n",
    "    \n",
    "    # index of the nearest neighbour for each data point in the unseen-data\n",
    "    str_neghbrs.append(tree.query(ftrs_estm.iloc[indx,:].values)[1])\n",
    "    \n",
    "    # distance of the nearest neighbour for each data point in the unseen-data\n",
    "    negbrs_dist.append(tree.query(ftrs_estm.iloc[indx,:].values)[0])\n",
    "    \n",
    "    # nearest neighbour for each data point in the unseen-data\n",
    "    negbrs_valus.append(A[tree.query(ftrs_estm.iloc[indx,:].values)[1]])\n",
    "    \n",
    "\n",
    "# Get the neighbour from the training sites for each site in the unseen data (data for estimation)\n",
    "sites_estimation['neighbouring_site']=sites_train.iloc[str_neghbrs,:]['site'].values\n",
    "\n",
    "sites_estimation['neighbour_distance']=negbrs_dist\n",
    "\n",
    "site_loc_buffer=gpd.read_file(data_folder+accesb_area_file)\n",
    "\n",
    "\n",
    "\n",
    "site_loc_buffer.geometry=site_loc_buffer.centroid\n",
    "\n",
    "site_loc_buffer=site_loc_buffer.to_crs(crs_deg)\n",
    "\n",
    "sites_estimation=sites_estimation[['site','neighbouring_site','neighbour_distance']].\\\n",
    "rename(columns={'site':'counter'}).merge(site_loc_buffer,on=['counter'],how='inner')\n",
    "\n",
    "sites_estimation.rename(columns={'counter':'site'},inplace=True)\n",
    "\n",
    "del sites_estimation['geom_type']\n",
    "del sites_estimation['area']\n",
    "\n",
    "sites_estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick sites in the unseen data which are most similar to the \n",
    "# sites in the training data \n",
    "\n",
    "sites_estimation=sites_estimation.sort_values(by='neighbour_distance',ascending=True).reset_index(drop=True)\n",
    "\n",
    "sites_estimation['neighbour_distance'].hist(bins=10)\n",
    "plt.show()\n",
    "\n",
    "sites_estimation=sites_estimation[sites_estimation['neighbour_distance'].\\\n",
    "                                  le(sites_estimation['neighbour_distance'].quantile(0.25))]\n",
    "\n",
    "sites_estimation['neighbour_distance'].hist(bins=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Number of sites in the data to be estimated {}'.format(sites_estimation['site'].unique().shape[0]))\n",
    "\n",
    "print('Number of sites in the training data most similar {}'.format(sites_estimation['neighbouring_site'].unique().shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation of all the counter locations\n",
    "ax=counter_ne.plot(color='blue',label='Natural England sites :{}'.\\\n",
    "                             format(counter_ne.shape[0]))\n",
    "contextily.add_basemap(ax,crs= crs_deg,source=contextily.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "counter_nd.plot(ax=ax,color='red',label='North downs way sites :{}'.\\\n",
    "                                   format(counter_nd.shape[0]))\n",
    "contextily.add_basemap(ax,crs= crs_deg,source=contextily.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "counter_cr.plot(ax=ax,color='green',label='Canal river trust sites :{}'.\\\n",
    "                                  format(counter_cr.shape[0]))\n",
    "contextily.add_basemap(ax,crs= crs_deg,source=contextily.providers.OpenStreetMap.Mapnik)\n",
    "\n",
    "\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualisation of sites most similar to the spatial sites in the unseen data\n",
    "\n",
    "sites_estimation.geometry=gpd.GeoDataFrame(sites_estimation).centroid.to_crs(crs_deg)\n",
    "sites_estimation=gpd.GeoDataFrame(sites_estimation)\n",
    "labels=sites_estimation['neighbouring_site'].values\n",
    "\n",
    "# Group unseen sites by the label of the neighbouring site\n",
    "hull=sites_estimation[[\"geometry\"]].to_crs(crs_deg).dissolve(by=labels).convex_hull\n",
    "\n",
    "trng_sites=pd.concat([counter_ne,counter_nd])\n",
    "\n",
    "trng_sites=trng_sites[trng_sites['counter'].isin(labels)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax=trng_sites.plot(color='green',label='Number of sites as potential labels for unseen data :{}'.\\\n",
    "                   format(trng_sites.shape[0]))\n",
    "contextily.add_basemap(ax,crs= crs_deg,source=contextily.providers.OpenStreetMap.Mapnik)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites_estimation.groupby('neighbouring_site')['site'].count().sort_values(ascending=False).plot(kind='barh')\n",
    "plt.xlabel('Number of similar sites in the unseen data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on top-n sites (in the training data) with the \n",
    "# highest count of  similar sites found in the unseen data\n",
    "\n",
    "most_similar_sites=list(sites_estimation.groupby('neighbouring_site')['site'].count().\\\n",
    "sort_values(ascending=False)[:10].index.values)\n",
    "\n",
    "sites_estimation=sites_estimation[sites_estimation['neighbouring_site'].isin(most_similar_sites)]\n",
    "\n",
    "labels=sites_estimation['neighbouring_site'].values\n",
    "\n",
    "# Group unseen sites by the label of the neighbouring site\n",
    "hull=sites_estimation[[\"geometry\"]].to_crs(crs_deg).dissolve(by=labels).convex_hull\n",
    "\n",
    "print(sites_estimation)\n",
    "trng_sites=trng_sites[trng_sites['counter'].isin(most_similar_sites)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trng_sites.to_pickle(data_folder+'crt_and_similar_sites.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "\n",
    "n_colors = len(trng_sites['counter'].unique())\n",
    "colours = cm.rainbow(np.linspace(0, 1, n_colors))\n",
    "\n",
    "# color code each site (in the training set) and same color \n",
    "# for similar site found in the unseen data\n",
    "\n",
    "color_dict = dict(zip(trng_sites['counter'].unique(),colours))\n",
    "\n",
    "trng_sites['Color'] = trng_sites['counter'].map(color_dict)\n",
    "\n",
    "sites_estimation['Color'] =sites_estimation['neighbouring_site'].map(color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise sites and clusters they belong to\n",
    "\n",
    "# Set up figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(5, 5))\n",
    "# Plot individual locations\n",
    "trng_sites.to_crs(crs_deg).plot(\n",
    "    # Colour by cluster label\n",
    "    column='counter',\n",
    "    # Consider label as categorical\n",
    "    categorical=False,\n",
    "    # Add 50% of transparency\n",
    "    alpha=0.95,\n",
    "    # Include legend\n",
    "    legend=True,\n",
    "    # Draw on axis `ax`\n",
    "    ax=ax,\n",
    "    # Use circle as marker\n",
    "    marker=\"o\",\n",
    "    # Position legend outside the map\n",
    "    legend_kwds={\"bbox_to_anchor\": (1, 1)},\n",
    ")\n",
    "\n",
    "sites_estimation.to_crs(crs_deg).plot(\n",
    "    # Colour by cluster label\n",
    "    column=labels,\n",
    "    # Consider label as categorical\n",
    "    categorical=True,\n",
    "    # Add 50% of transparency\n",
    "    alpha=0.85,\n",
    "    # Include legend\n",
    "    legend=False,\n",
    "    # Draw on axis `ax`\n",
    "    ax=ax,\n",
    "    # Use circle as marker\n",
    "    marker=\"+\",\n",
    "    # Position legend outside the map\n",
    "    legend_kwds={\"bbox_to_anchor\": (1, 1)},\n",
    ")\n",
    "\n",
    "\n",
    "# Plot convex hull polygons for each cluster label\n",
    "# except that for -1 (observations classified as noise)\n",
    "#hull.boundary.plot(color=\"k\", ax=ax)\n",
    "# Add basemap\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=sites_estimation.to_crs(crs_deg).crs.to_string(),\n",
    "    source=contextily.providers.CartoDB.VoyagerNoLabels,\n",
    ")\n",
    "plt.title(label='Number of most similar sites in the un-seen data {} marked with +'.\\\n",
    "          format(sites_estimation['site'].unique().shape[0]))\n",
    "# Remove axes\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure and axis\n",
    "f, ax = plt.subplots(1, figsize=(10, 5))\n",
    "# Plot individual locations\n",
    "\n",
    "trng_sites.to_crs(crs_deg).plot(\n",
    "    # Colour by cluster label\n",
    "    column='counter',\n",
    "    #color=trng_sites['Color'],\n",
    "    cmap=colors.ListedColormap(list(color_dict.values())),\n",
    "    # Consider label as categorical\n",
    "    #categorical=False,\n",
    "    # Add 50% of transparency\n",
    "    alpha=0.95,\n",
    "    # Include legend\n",
    "    legend=True,\n",
    "    # Draw on axis `ax`\n",
    "    ax=ax,\n",
    "    # Use circle as marker\n",
    "    marker=\"o\",\n",
    "    # Position legend outside the map\n",
    "    legend_kwds={\"bbox_to_anchor\": (1, 1)},\n",
    ")\n",
    "\n",
    "sites_estimation.to_crs(crs_deg).plot(\n",
    "    # Colour by cluster label\n",
    "    column='site',\n",
    "    #color=trng_sites['Color'],\n",
    "    cmap=colors.ListedColormap(list(sites_estimation['Color'].values)),\n",
    "    # Consider label as categorical\n",
    "    #categorical=False,\n",
    "    # Add 50% of transparency\n",
    "    alpha=0.95,\n",
    "    # Include legend\n",
    "    legend=False,\n",
    "    # Draw on axis `ax`\n",
    "    ax=ax,\n",
    "    # Use circle as marker\n",
    "    marker=\"+\",\n",
    "    # Position legend outside the map\n",
    "    legend_kwds={\"bbox_to_anchor\": (1, 1)},\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# Plot convex hull polygons for each cluster label\n",
    "# except that for -1 (observations classified as noise)\n",
    "#hull.boundary.plot(color=\"k\", ax=ax)\n",
    "# Add basemap\n",
    "contextily.add_basemap(\n",
    "    ax,\n",
    "    crs=sites_estimation.to_crs(crs_deg).crs.to_string(),\n",
    "    source=contextily.providers.CartoDB.VoyagerNoLabels,\n",
    ")\n",
    "# Remove axes\n",
    "plt.title(label='Number of most similar sites in the un-seen data {} marked with +'.\\\n",
    "          format(sites_estimation['site'].unique().shape[0]))\n",
    "ax.set_axis_off();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model inferences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_estimation=pd.read_pickle(data_folder+'df_for_model_estimation.pkl')\n",
    "\n",
    "\n",
    "data_for_estimation=data_for_estimation[list(ftrs_to_keep)+['Date','site']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "data_for_estimation=data_for_estimation[data_for_estimation['site'].isin(list(sites_estimation['site'].\\\n",
    "                                                                              unique()))].reset_index(drop=True)\n",
    "\n",
    "\n",
    "identf_cols_to_be_added=data_for_estimation[['Date','site']]\n",
    "\n",
    "\n",
    "ftrs_model=[x for x in data_for_estimation.columns if x not in ['Date','site',target]]\n",
    "\n",
    "del data_for_estimation[target]\n",
    "\n",
    "\n",
    "data_used_for_training=pd.read_pickle(data_folder+'df_for_model_training.pkl')\n",
    "\n",
    "\n",
    "data_used_for_training=data_used_for_training[list(ftrs_to_keep)+['Date','site']].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "data_used_for_training=data_used_for_training[data_used_for_training['site'].isin(most_similar_sites)].\\\n",
    "reset_index(drop=True)\n",
    "\n",
    "identf_cols_to_be_added_train=data_used_for_training[['Date','site',target]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use trained model to make predictions:\n",
    "\n",
    "pycaret_flag=True\n",
    "# If using py-caret model\n",
    "\n",
    "if pycaret_flag:\n",
    "    \n",
    "    pycaret_model = load_model(data_folder+'pycaret_regression_model') \n",
    "    \n",
    "    \n",
    "    \n",
    "    estimation_trained_model = predict_model(pycaret_model, data=data_for_estimation[ftrs_model])\n",
    "    \n",
    "    estimation_trained_model['prediction_label']=estimation_trained_model['prediction_label'].clip(0)\n",
    "    \n",
    "    prediction_trained_model = predict_model(pycaret_model, data=data_used_for_training[ftrs_model])\n",
    "    \n",
    "    prediction_trained_model['prediction_label']=prediction_trained_model['prediction_label'].clip(0)\n",
    "\n",
    "\n",
    "# If using sk-learn model   \n",
    "    \n",
    "sklrn_flag=False\n",
    "\n",
    "if sklrn_flag:\n",
    "    \n",
    "    model_regularisation = pickle.load(open(data_folder+'model_regularisation.pkl', 'rb'))\n",
    "    # load the scaler\n",
    "    \n",
    "    scaler = pickle.load(open(data_folder+'scaler.pkl', 'rb'))\n",
    "    \n",
    "    # using  trained model sklearn to infer visitors at 'new but similar' locations\n",
    "    estimation_trained_model=data_for_estimation[ftrs_model].copy()\n",
    "\n",
    "    df_estmn=data_for_estimation[ftrs_model].copy() \n",
    "\n",
    "    features_estmn = [x for x in df_estmn.columns if x not in ['Date','site',target]]\n",
    "\n",
    "    df_estmn[features_estmn] = scaler.transform(df_estmn[features_estmn])\n",
    "\n",
    "    X_estm = df_estmn[features_estmn].values.reshape(-1, len(features_estmn))\n",
    "\n",
    "    y_estm= model_regularisation.predict(X_estm)\n",
    "\n",
    "    y_estm =[max(0,x) for x in y_estm]\n",
    "\n",
    "\n",
    "    estimation_trained_model['prediction_label']=y_estm\n",
    "    \n",
    "    \n",
    "    \n",
    "    # using  trained model sklearn\n",
    "    \n",
    "    prediction_trained_model=data_used_for_training[ftrs_model].copy()\n",
    "\n",
    "    df_trng=data_used_for_training[ftrs_model].copy() \n",
    "    \n",
    "    \n",
    "\n",
    "    features_trng = [x for x in df_trng.columns if x not in ['Date','site',target]]\n",
    "\n",
    "    df_trng[features_trng] = scaler.transform(df_trng[features_trng])\n",
    "\n",
    "    X_trng = df_trng[features_estmn].values.reshape(-1, len(features_trng))\n",
    "\n",
    "    y_trng= model_regularisation.predict(X_trng)\n",
    "\n",
    "    y_trng =[max(0,x) for x in y_trng]\n",
    "\n",
    "\n",
    "    prediction_trained_model['prediction_label']=y_trng\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "estimation_trained_model=estimation_trained_model.assign(Date=identf_cols_to_be_added['Date'],\\\n",
    "                                                         site=identf_cols_to_be_added['site'])\n",
    "\n",
    "estimation_trained_model=estimation_trained_model[['total_trip_count','Date','prediction_label','site']]\n",
    "\n",
    "\n",
    "\n",
    "prediction_trained_model=prediction_trained_model.assign(Date=identf_cols_to_be_added_train['Date'],\\\n",
    "                                                         site=identf_cols_to_be_added_train['site'],\\\n",
    "                                                         people_counter_data=identf_cols_to_be_added_train[target])\n",
    "\n",
    "prediction_trained_model=prediction_trained_model[['total_trip_count',target,'Date','prediction_label','site']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimation_trained_model.to_pickle(data_folder+'estimation_trained_model.pkl')\n",
    "\n",
    "prediction_trained_model.to_pickle(data_folder+'prediction_trained_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat-map for a specific season\n",
    "estimation_trained_model['season']=estimation_trained_model['Date'].apply(lambda x: x.split('-')[1]).\\\n",
    "apply(lambda x : get_season(x))\n",
    "est_sesn=estimation_trained_model[estimation_trained_model['season']=='summer']\n",
    "\n",
    "est_sesn=est_sesn.groupby('site')[['total_trip_count','prediction_label']].mean().reset_index()\n",
    "\n",
    "est_sesn=est_sesn.merge(counter_cr,left_on=['site'],right_on='counter',how='inner')\n",
    "\n",
    "del est_sesn['counter']\n",
    "\n",
    "\n",
    "\n",
    "# Estimated\n",
    "fig = px.density_mapbox(est_sesn, lat='latitude', lon='longitude', z='prediction_label',\n",
    "                        mapbox_style=\"stamen-terrain\")\n",
    " \n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Strava\n",
    "fig = px.density_mapbox(est_sesn, lat='latitude', lon='longitude', z='total_trip_count',\n",
    "                        mapbox_style=\"stamen-terrain\")\n",
    " \n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualisations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average value across all sites\n",
    "\n",
    "estimation_ntnl=estimation_trained_model.groupby('Date')[['total_trip_count','prediction_label']].mean()\n",
    "\n",
    "\n",
    "\n",
    "prediction_trn_ntnl=prediction_trained_model.groupby('Date')[['total_trip_count',target,'prediction_label']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=estimation_ntnl['prediction_label'], x=estimation_ntnl.index, name=\"Estimated data\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=estimation_ntnl['total_trip_count'], x=estimation_ntnl.index, name=\"Strava data\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Estimated visitor count for similar sites\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"<b>Mean estimated count</b>\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"<b>Mean Strava count</b>\", secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = estimation_ntnl[['prediction_label']].copy()\n",
    "\n",
    "\n",
    "decompose_result_mult = seasonal_decompose(analysis, model=\"multiplicative\",period=4)\n",
    "\n",
    "trend = decompose_result_mult.trend\n",
    "seasonal = decompose_result_mult.seasonal\n",
    "residual = decompose_result_mult.resid\n",
    "\n",
    "#decompose_result_mult.plot();\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=analysis['prediction_label'], x=analysis.index, name=\"Predicted data\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=trend.values,x=trend.index, name=\"Predicted trend\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Estimated visitor count for similar sites\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"<b>Visitor count</b>\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"<b>Visitor trend</b>\", secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=prediction_trn_ntnl['prediction_label'], x=prediction_trn_ntnl.index, name=\"Predicted data\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=prediction_trn_ntnl[target],x=prediction_trn_ntnl.index, name=\"Actual data\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=prediction_trn_ntnl['total_trip_count'], x=prediction_trn_ntnl.index, name=\"Strava data\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Visitor count for sites in the training data\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"<b>Mean count</b>\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"<b>Mean Strava count</b>\", secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = prediction_trn_ntnl[['prediction_label']].copy()\n",
    "\n",
    "\n",
    "decompose_result_mult = seasonal_decompose(analysis, model=\"additive\",period=4)\n",
    "\n",
    "trend = decompose_result_mult.trend\n",
    "seasonal = decompose_result_mult.seasonal\n",
    "residual = decompose_result_mult.resid\n",
    "\n",
    "#decompose_result_mult.plot();\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=trend, x=analysis.index, name=\"Predicted trend\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "\n",
    "analysis = prediction_trn_ntnl[[target]].copy()\n",
    "\n",
    "\n",
    "decompose_result_mult = seasonal_decompose(analysis, model=\"multiplicative\",period=4)\n",
    "\n",
    "trend = decompose_result_mult.trend\n",
    "seasonal = decompose_result_mult.seasonal\n",
    "residual = decompose_result_mult.resid\n",
    "\n",
    "#decompose_result_mult.plot();\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=trend.values,x=trend.index, name=\"Actual trend\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Visitor count for sites in the training data\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"<b>Predicted trend</b>\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"<b>Actual trend</b>\", secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites_count=pd.read_pickle(data_folder+'complete_dataset.pkl')\n",
    "\n",
    "\n",
    "all_sites_count=all_sites_count[all_sites_count['site'].isin(places_ne+places_nd+places_cr)]\n",
    "\n",
    "all_sites_count_avg=all_sites_count.groupby('Date')['total_trip_count'].sum()\n",
    "\n",
    "analysis = all_sites_count_avg.values\n",
    "\n",
    "\n",
    "decompose_result_mult = seasonal_decompose(analysis, model=\"multiplicative\",period=4)\n",
    "\n",
    "trend = decompose_result_mult.trend\n",
    "seasonal = decompose_result_mult.seasonal\n",
    "residual = decompose_result_mult.resid\n",
    "\n",
    "#decompose_result_mult.plot();\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=trend, x=all_sites_count_avg.index, name=\"Trend\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "\n",
    "analysis = all_sites_count_avg.copy()\n",
    "\n",
    "\n",
    "\n",
    "#decompose_result_mult.plot();\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=all_sites_count_avg.values,x=all_sites_count_avg.index, name=\"Observed count\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Mean Strava count for all sites\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"<b>Observed trend</b>\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"<b>Observed data</b>\", secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites_count_avg=all_sites_count.groupby('Date')[target].mean()\n",
    "\n",
    "analysis = all_sites_count_avg.values\n",
    "\n",
    "\n",
    "decompose_result_mult = seasonal_decompose(analysis, model=\"multiplicative\",period=6)\n",
    "\n",
    "trend = decompose_result_mult.trend\n",
    "seasonal = decompose_result_mult.seasonal\n",
    "residual = decompose_result_mult.resid\n",
    "\n",
    "#decompose_result_mult.plot();\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=trend, x=all_sites_count_avg.index, name=\"Trend\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "\n",
    "analysis = all_sites_count_avg.copy()\n",
    "\n",
    "\n",
    "\n",
    "#decompose_result_mult.plot();\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=all_sites_count_avg.values,x=all_sites_count_avg.index, name=\"Observed count\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Mean visitor count for all sites\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"<b>Observed trend</b>\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"<b>Observed data</b>\", secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sites_count=pd.read_pickle(nature_srvy_visits)\n",
    "\n",
    "all_sites_count.rename(columns={'counter':'site'},inplace=True)\n",
    "\n",
    "all_sites_count=all_sites_count[all_sites_count['site'].isin(places_ne+places_nd+places_cr)]\n",
    "\n",
    "all_sites_count_avg=all_sites_count.groupby('Date')['No_Of_Visits'].mean()\n",
    "\n",
    "analysis = all_sites_count_avg\n",
    "\n",
    "\n",
    "decompose_result_mult = seasonal_decompose(analysis, model=\"multiplicative\",period=4)\n",
    "\n",
    "trend = decompose_result_mult.trend\n",
    "seasonal = decompose_result_mult.seasonal\n",
    "residual = decompose_result_mult.resid\n",
    "\n",
    "all_sites_count_avg.index=all_sites_count_avg.index.astype(str)\n",
    "\n",
    "#decompose_result_mult.plot();\n",
    "\n",
    "# Create figure with secondary y-axis\n",
    "fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "\n",
    "# Add traces\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=trend, x=all_sites_count_avg.index, name=\"Trend\"),\n",
    "    secondary_y=False,\n",
    ")\n",
    "\n",
    "\n",
    "analysis = all_sites_count_avg.copy()\n",
    "\n",
    "\n",
    "\n",
    "#decompose_result_mult.plot();\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(y=all_sites_count_avg.values,x=all_sites_count_avg.index, name=\"Observed count\"),\n",
    "    secondary_y=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Add figure title\n",
    "fig.update_layout(\n",
    "    title_text=\"Mean People and Nature Survey count for all sites\"\n",
    ")\n",
    "\n",
    "# Set x-axis title\n",
    "fig.update_xaxes(title_text=\"Time\")\n",
    "\n",
    "# Set y-axes titles\n",
    "fig.update_yaxes(title_text=\"<b>Observed trend</b>\", secondary_y=False)\n",
    "fig.update_yaxes(title_text=\"<b>Observed data</b>\", secondary_y=True)\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "req_old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "73dd042dec22895802a5cf4c230cd0d0aa33a4e312107f26490806e8c532eb8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
