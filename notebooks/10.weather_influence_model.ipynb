{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c51bfab",
   "metadata": {},
   "source": [
    "In this notebook we explore the effect of weather in influencing the visitor numbers.\n",
    "\n",
    "We make use of Fixed effects approach to correctly account (control) for static infleunces while exploring the effect of the dynamic variables.\n",
    "\n",
    "https://matheusfacure.github.io/python-causality-handbook/14-Panel-Data-and-Fixed-Effects.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8bdcdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e4cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# All the variables are defined in the Config file\n",
    "from model_config import *\n",
    "from model_packages import *\n",
    "from model_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0945e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_counter_strava=pd.read_pickle(data_folder+'strava_data_all_sites.pkl')\n",
    "\n",
    "weather_df_all_sites=pd.read_pickle(data_folder+'weather_df.pkl')\n",
    "\n",
    "weather_df_all_sites['Date']=weather_df_all_sites['Date'].astype(str)\n",
    "\n",
    "data_counter_strava.dropna().groupby('Date')['site'].count().plot(style='-o')\n",
    "plt.ylabel('Number of monitoring sites')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c501e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_counter_strava.merge(weather_df_all_sites,on=['Date','site'],how='inner').groupby('Date')['site'].count().plot(style='-o')\n",
    "plt.ylabel('Number of monitoring sites with feature list available')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c3561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_training_sites=data_counter_strava.merge(weather_df_all_sites,on=['Date','site'],how='inner').dropna()\n",
    "\n",
    "data_with_training_sites['year']=data_with_training_sites['Date'].apply(lambda x: int(x.split('-')[0]))\n",
    "\n",
    "data_with_training_sites['Date']=data_with_training_sites['Date'].apply(lambda x: int(x.split('-')[1]))\n",
    "\n",
    "\n",
    "data_with_training_sites['season']=data_with_training_sites['Date'].apply(lambda x : get_season(x))\n",
    "\n",
    "\n",
    "data_with_training_sites=data_with_training_sites.sort_values(by=['site']).reset_index(drop=True)\n",
    "\n",
    "data_with_training_sites.rename(columns={'Date':'Month'},inplace=True)\n",
    "\n",
    "data_with_training_sites['Month']=data_with_training_sites['Month'].map(dict(zip(range(1,13),\\\n",
    "                                                                           [calendar.month_abbr[x]\\\n",
    "                                                                            for x in range(1,13)])))\n",
    "\n",
    "data_with_training_sites.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087e76fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data_with_training_sites.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391e057b",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_names=[x for x in data['site'].unique()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9285947a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of year:\n",
    "\n",
    "# Regressing against a dummy variable (the only predictor) \n",
    "# is the same as getting mean of the output\n",
    "# variable grouped by that dummy variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b3bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary least square approach-neglects the panel data\n",
    "# structure of the data (repeated observations)\n",
    "mod = smf.ols(\"people_counter_data ~ C(year)\", data=data).fit()\n",
    "print(mod.summary().tables[1])\n",
    "\n",
    "# Get the mean grouped by year\n",
    "data.groupby(\"year\")[\"people_counter_data\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8338f90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effect of year:\n",
    "\n",
    "mod = smf.ols(\"people_counter_data ~ C(season)\", data=data).fit()\n",
    "print(mod.summary().tables[1])\n",
    "\n",
    "data.groupby(\"season\")[\"people_counter_data\"].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ee86a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# variables which are not-static\n",
    "#data.groupby(\"site\").std().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b6856c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data[['site','people_counter_data','tavg','year','total_trip_count','season','Month']]\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7dcb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed effects approach \n",
    "\n",
    "# https://matheusfacure.github.io/python-causality-handbook/14-Panel-Data-and-Fixed-Effects.html\n",
    "\n",
    "Y = \"people_counter_data\" # outcome variable\n",
    "T = \"total_trip_count\" # treatment\n",
    "X = [T]+['tavg'] #list of predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ebf2b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the mean observation for each site:\n",
    "mean_data = data.groupby([\"site\"])[X+[Y]].mean()\n",
    "\n",
    "#de-mean data\n",
    "\n",
    "demeaned_data = (data\n",
    "               .set_index([\"site\"]) # set the index as the site indicator\n",
    "               [X+[Y]]\n",
    "               - mean_data) # subtract the mean data\n",
    "\n",
    "# ols on de-mean data- this way we control for all fixed static influences\n",
    "mod = smf.ols(f\"{Y} ~ {'+'.join(X)}\", data=demeaned_data).fit()\n",
    "mod.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea3d1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Panel-data out of box approach: identical results to the above result\n",
    "mod = PanelOLS.from_formula(f\"{Y} ~ {'+'.join(X)}\"+'+ EntityEffects',data=data.set_index([\"site\",\"year\"]))\n",
    "\n",
    "result = mod.fit(cov_type='clustered', cluster_entity=True)\n",
    "result.summary.tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a21766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If adding a dummy for each individual controls for fixed site characteristics, \n",
    "# adding a time dummy would control for variables that are fixed for each time period,\n",
    "# but that might change across time. e.g. covid restrictions\n",
    "mod = PanelOLS.from_formula(f\"{Y} ~ {'+'.join(X)}\"+'+ EntityEffects+TimeEffects',\n",
    "                            data=data.set_index([\"site\",\"year\"]),drop_absorbed=True)\n",
    "\n",
    "result = mod.fit(cov_type='clustered', cluster_entity=True)\n",
    "result.summary.tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59db4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinary-least square approach-has potential bias because\n",
    "# of omitted variables\n",
    "\n",
    "mod = smf.ols(\"people_counter_data ~ tavg+total_trip_count\", data=data).fit()\n",
    "mod.summary().tables[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d178295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in-sample predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2655b16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predtnc=result.predict(fitted=True,effects=True,idiosyncratic=True).reset_index()\n",
    "\n",
    "predtnc['Month']=data['Month'].map(dict(zip([calendar.month_abbr[x] for x in range(1,13)],range(1,13))))\n",
    "\n",
    "predtnc['Date']=predtnc['Month'].astype(str)+'-'+predtnc['year'].astype(str)\n",
    "\n",
    "predtnc['people_counter_data']=data['people_counter_data']\n",
    "\n",
    "predtnc['Date']=pd.to_datetime(predtnc['Date'])#\n",
    "\n",
    "predtnc['season']=data['season']\n",
    "\n",
    "model_actls=predtnc.groupby(['Date'])['people_counter_data'].mean().plot(label='Counter data')\n",
    "model_predctns=predtnc.groupby(['Date'])['fitted_values'].mean().plot(label='In sample predictions')\n",
    "plt.legend()\n",
    "\n",
    "\n",
    "predtnc=predtnc.groupby(['season','site'])[['people_counter_data','fitted_values']].mean().reset_index()\n",
    "#Visualisation of the time series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e6e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add counter locations\n",
    "\n",
    "sites_df=gpd.read_file(data_folder+'accessibility.shp')\n",
    "\n",
    "sites_df=sites_df[sites_df['geom_type']=='5km buffer'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "predtnc=predtnc.merge(sites_df,left_on=['site'],right_on=['counter'],how='inner')\\\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "predtnc.geometry=gpd.GeoDataFrame(predtnc).centroid.to_crs(crs_deg)\n",
    "\n",
    "predtnc['latitude'] = predtnc.geometry.apply(lambda p: p.y)\n",
    "predtnc['longitude'] =predtnc.geometry.apply(lambda p: p.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# heat-map for a specific season\n",
    "\n",
    "predtnc_sesn=predtnc[predtnc['season']=='summer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284683a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Actual\n",
    "fig = px.density_mapbox(predtnc_sesn, lat='latitude', lon='longitude', z='people_counter_data',\n",
    "                        mapbox_style=\"stamen-terrain\")\n",
    " \n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c259e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction\n",
    "fig = px.density_mapbox(predtnc_sesn, lat='latitude', lon='longitude', z='fitted_values',\n",
    "                        mapbox_style=\"stamen-terrain\")\n",
    " \n",
    "fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b77c6a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Controlling for seasons:\n",
    "\n",
    "for sesn in list(data['season'].unique()):\n",
    "    #data_sesn=data[data['season']==sesn]\n",
    "    scaler = RobustScaler()\n",
    "    \n",
    "    data_sesn=data[data['season']==sesn].reset_index(drop=True)\n",
    "    \n",
    "    data_sesn_scln=data_sesn[X+[Y]].copy()\n",
    "    \n",
    "    cols=data_sesn_scln.columns\n",
    "    \n",
    "    \n",
    "    # we are developing a separate model for each season, so we need to standardise the coefs\n",
    "    data_sesn_scln=scaler.fit_transform(data_sesn_scln)\n",
    "    \n",
    "    data_sesn_scln=pd.DataFrame(data_sesn_scln,columns=cols)\n",
    "    \n",
    "    \n",
    "    \n",
    "    data_sesn[X+[Y]]=data_sesn_scln\n",
    "    \n",
    "    print('Fixed effects model for {}'.format(sesn))\n",
    "    #mod = PanelOLS.from_formula(f\"{Y} ~ {'+'.join(X)}\"+'+ EntityEffects',\n",
    "    #                        data=data_sesn.set_index([\"site\",\"year\"]),drop_absorbed=True)\n",
    "    #result = mod.fit(cov_type='clustered', cluster_entity=True)\n",
    "    \n",
    "    \n",
    "    #print(result.summary.tables[1])\n",
    "    \n",
    "    \n",
    "    #from linearmodels.panel import PanelOLS\n",
    "    \n",
    "    mean_data = data_sesn.groupby([\"site\"])[X+[Y]].mean()\n",
    "    \n",
    "    demeaned_data = (data_sesn\n",
    "               .set_index([\"site\"]) # set the index as the site indicator\n",
    "               [X+[Y]]\n",
    "               - mean_data) # subtract the mean data\n",
    "    \n",
    "    mod = smf.ols(f\"{Y} ~ {'+'.join(X)}\", data=demeaned_data).fit()\n",
    "    \n",
    "    print(mod.summary().tables[1])\n",
    "    print('+'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95463515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at the spatial variation in \n",
    "# the effect of weather/strava count \n",
    "\n",
    "data_sites=data.reset_index()\n",
    "# Evaluating the influence of weather for each site\n",
    "\n",
    "concat_coef=[]\n",
    "\n",
    "for indv_site in list(data_sites['site'].unique()):\n",
    "    \n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    data_site=data_sites[data_sites['site']==indv_site]\n",
    "    \n",
    "    data_site=data_site[X+[Y]]\n",
    "    \n",
    "    cols=data_site.columns\n",
    "    \n",
    "    # standardise the data if we have to compare regression coeffs \n",
    "    # across mutliple sites\n",
    "    data_site=scaler.fit_transform(data_site)\n",
    "    \n",
    "    data_site=pd.DataFrame(data_site,columns=cols)\n",
    "    \n",
    "    X_regr,y_regr=data_site[X].copy(), data_site[Y].copy()\n",
    "    \n",
    "    # Getting regression coefficients as table\n",
    "    lm = pg.linear_regression(pd.DataFrame(X_regr,columns=X), y_regr)\n",
    "    \n",
    "    lm['site']=indv_site\n",
    "    \n",
    "    concat_coef.append(lm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dc0d33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547716cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "var='total_trip_count'#'tavg'#\n",
    "\n",
    "coef_all_sites=pd.concat(concat_coef)\n",
    "coef_all_sites=coef_all_sites[coef_all_sites['names']==var].sort_values(by='coef')\n",
    "\n",
    "# only the non-significant coefs\n",
    "coef_all_sites_ns=coef_all_sites[coef_all_sites['pval']>=0.1]\n",
    "\n",
    "# only the significant coefs\n",
    "coef_all_sites=coef_all_sites[coef_all_sites['pval']<0.1]\n",
    "\n",
    "coef_all_sites=coef_all_sites[coef_all_sites['coef']>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc29468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f315b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression coefs\n",
    "coef_all_sites.set_index('site').sort_values(by='coef')['coef'].plot(kind='barh',figsize=(10,5))\n",
    "plt.xlabel('{} (Regression cofficient)'.format(var))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75574e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spatial location of the sites\n",
    "\n",
    "sites_df=gpd.read_file(data_folder+'accessibility.shp')\n",
    "\n",
    "sites_df=sites_df[sites_df['geom_type']=='5km buffer'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "coef_all_sites_geo=coef_all_sites.set_index('site').sort_values(by='coef').reset_index().merge(sites_df,left_on=['site'],\\\n",
    "right_on=['counter'],how='inner')[['site','coef','pval','geometry']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "coef_all_sites_geo.geometry=gpd.GeoDataFrame(coef_all_sites_geo).centroid.to_crs(crs_deg)\n",
    "\n",
    "coef_all_sites_geo['latitude'] = coef_all_sites_geo.geometry.apply(lambda p: p.y)\n",
    "coef_all_sites_geo['longitude'] = coef_all_sites_geo.geometry.apply(lambda p: p.x)\n",
    "\n",
    "#coef_all_sites_geo['abs_coef']=np.abs(coef_all_sites_geo['coef'])\n",
    "\n",
    "# Visualisation of mean dog occupancy and number of visits.\n",
    "fig = px.scatter_mapbox(coef_all_sites_geo, lat=\"latitude\", lon=\"longitude\",\\\n",
    "                        color=\"coef\", size=\"coef\",\n",
    "                        color_continuous_scale=\"RdYlGn_r\",\n",
    "                        center={\"lat\": coef_all_sites_geo['latitude'].mean(),\\\n",
    "                                \"lon\": coef_all_sites_geo['longitude'].mean()}, zoom=3.5,\n",
    "                        mapbox_style=\"carto-positron\", hover_name=\"site\")\n",
    "fig.update_layout(margin=dict(l=0, r=0, t=30, b=10))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe9a88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means clustering on the sites\n",
    "\n",
    "df=sites_df.copy()\n",
    "df.geometry=df.geometry.centroid\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['lon'] = df.geometry.apply(lambda p: p.x)\n",
    "df['lat'] = df.geometry.apply(lambda p: p.y)\n",
    "\n",
    "\n",
    "sites_geom_tvg=df.merge(coef_all_sites_geo[['site','coef']],left_on=['counter'],right_on=['site'],how='inner')\n",
    "\n",
    "df=sites_geom_tvg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6323a2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the optimal cluster\n",
    "\n",
    "K_clusters = range(1,10)\n",
    "kmeans = [KMeans(n_clusters=i) for i in K_clusters]\n",
    "Y_axis = df[['lat']]\n",
    "X_axis = df[['lon']]\n",
    "score = [kmeans[i].fit(Y_axis).score(Y_axis) for i in range(len(kmeans))]\n",
    "# Visualize\n",
    "plt.plot(K_clusters, score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "K_clusters = range(1,10)\n",
    "kmeans = [KMeans(n_clusters=i) for i in K_clusters]\n",
    "lat_long = df[['lat','lon']]\n",
    "lot_size = df['coef']\n",
    "sample_weight = lot_size\n",
    "score = [kmeans[i].fit(lat_long, sample_weight = lot_size).score(lat_long) for i in range(len(kmeans))]\n",
    "plt.plot(K_clusters, score)\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Elbow Curve = Weighted')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8aaf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 2, init ='k-means++')\n",
    "kmeans.fit(df[['lon','lat']]) # Compute k-means clustering.\n",
    "df['cluster_label'] = kmeans.fit_predict(df[['lon','lat']])\n",
    "centers = kmeans.cluster_centers_ # Coordinates of cluster centers.\n",
    "labels = kmeans.predict(df[['lon','lat']]) # Labels of each point\n",
    "ax=df.plot.scatter(y = 'lat', x = 'lon', c=labels, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n",
    "world = gpd.read_file(world_boundaries)\n",
    "\n",
    "uk = world[world.name == 'U.K. of Great Britain and Northern Ireland'] \n",
    "\n",
    "uk=uk.to_crs(crs_mtr)\n",
    "\n",
    "uk.plot(ax=ax,alpha=0.1)\n",
    "plt.title('Unweighted k-means')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11f6abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters = 3, max_iter=1000, init ='k-means++')\n",
    "lat_long = df[['lon','lat']]\n",
    "lot_size = df['coef']\n",
    "weighted_kmeans_clusters = kmeans.fit(lat_long, sample_weight = lot_size) # Compute k-means clustering.\n",
    "df['cluster_label'] = kmeans.predict(lat_long, sample_weight = lot_size)\n",
    "centers = kmeans.cluster_centers_ # Coordinates of cluster centers.\n",
    "labels = df['cluster_label'] # Labels of each point\n",
    "ax=df.plot.scatter(y = 'lat', x = 'lon', c=labels, s=50, cmap='viridis')\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5)\n",
    "\n",
    "\n",
    "uk.plot(ax=ax,alpha=0.1)\n",
    "plt.title('Weighted k-means clustering based on {}'.format(var),fontsize=12, fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa8074",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the people counter data\n",
    "\n",
    "national_trail_count=prepare_counter_data(ne_strava_data_file,cnt_ct_off_yr_ne)\n",
    "\n",
    "\n",
    "# Get the people counter data\n",
    "\n",
    "north_downs_count=prepare_counter_data(ndw_strava_data_file,cnt_ct_off_yr_nd)\n",
    "\n",
    "# North Downs Way has no data for 2020\n",
    "\n",
    "df_count=pd.concat([national_trail_count,north_downs_count],axis=1)#.dropna()\n",
    "\n",
    "df_count_smooth_no_na=df_count\n",
    "\n",
    "\n",
    "df_count_smooth_no_na.columns=[x.replace(\"  \",\" \").replace(\" \",\"_\") for x in df_count_smooth_no_na.columns]\n",
    "\n",
    "#Data wrangling on counter data\n",
    "df_count_smooth_no_na=df_count_smooth_no_na.T.stack().reset_index()\n",
    "\n",
    "df_count_smooth_no_na.rename(columns={'level_0':'site',0:'people_count'},inplace=True)\n",
    "\n",
    "df_count_smooth_no_na['Date']=df_count_smooth_no_na['Date'].dt.to_period('M').astype(str)\n",
    "\n",
    "df_count_smooth_no_na\n",
    "\n",
    "# Get the weather data prepared in separate notebooks\n",
    "\n",
    "weather_df_all_sites=pd.read_pickle(data_folder+'weather_df.pkl')\n",
    "\n",
    "weather_df_all_sites['Date']=weather_df_all_sites['Date'].astype(str)\n",
    "\n",
    "\n",
    "# Merge weather data and counter data\n",
    "df_count_smooth_no_na=df_count_smooth_no_na.merge(weather_df_all_sites,on=['site','Date'],how='inner')\n",
    "\n",
    "df_count_smooth_no_na['year']=df_count_smooth_no_na['Date'].apply(lambda x: int(x.split('-')[0]))\n",
    "\n",
    "df_count_smooth_no_na['Date']=df_count_smooth_no_na['Date'].apply(lambda x: int(x.split('-')[1]))\n",
    "\n",
    "\n",
    "df_count_smooth_no_na['season']=df_count_smooth_no_na['Date'].apply(lambda x : get_season(x))\n",
    "\n",
    "\n",
    "df_count_smooth_no_na=df_count_smooth_no_na.sort_values(by=['site']).reset_index(drop=True)\n",
    "\n",
    "df_count_smooth_no_na.rename(columns={'Date':'Month'},inplace=True)\n",
    "\n",
    "df_count_smooth_no_na['Month']=df_count_smooth_no_na['Month'].map(dict(zip(range(1,13),\\\n",
    "                                                                           [calendar.month_abbr[x]\\\n",
    "                                                                            for x in range(1,13)])))\n",
    "\n",
    "df_count_smooth_no_na.sample(5)\n",
    "\n",
    "\n",
    "# Get the site names\n",
    "\n",
    "data=df_count_smooth_no_na.copy()\n",
    "\n",
    "\n",
    "\n",
    "data=pd.read_pickle(data_folder+'complete_dataset.pkl')\n",
    "data.rename(columns={'people_counter_data':'people_count'},inplace=True)\n",
    "\n",
    "data['year']=data.Date.apply(lambda x: int(x.split('-')[0]))\n",
    "\n",
    "data=data.dropna()\n",
    "\n",
    "data['Date']=data['Date'].apply(lambda x: int(x.split('-')[1]))\n",
    "\n",
    "\n",
    "data['season']=data['Date'].apply(lambda x : get_season(x))\n",
    "\n",
    "\n",
    "data=data.sort_values(by=['site']).reset_index(drop=True)\n",
    "\n",
    "data.rename(columns={'Date':'Month'},inplace=True)\n",
    "\n",
    "data['Month']=data['Month'].map(dict(zip(range(1,13),[calendar.month_abbr[x] for x in range(1,13)])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyenv_natural_spaces",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9 (main, Jan 11 2023, 14:59:37) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "ed2dafe288f5e43b614874fe546a312150df24fd5a7d2e0f41b63eb50bff097b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
